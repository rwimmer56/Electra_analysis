{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_ans= pd.read_csv('has_answer_newsqa.csv')\n",
    "has_ans= has_ans.drop(has_ans.columns[0], axis=1)\n",
    "has_ans= has_ans.drop(has_ans.columns[6], axis=1)\n",
    "has_ans['possible_answer'] = [x[1:-1] for x in has_ans['possible_answer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "has_ans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_ans['answer_text_in_prediction'] = has_ans.apply(lambda x: x.pred_answer in x.possible_answer, axis=1)\n",
    "has_ans['answer_prediction_in_answer_text'] = has_ans.apply(lambda x: x.possible_answer in x.pred_answer, axis=1)\n",
    "has_ans['TF_F1']= has_ans.answer_prediction_in_answer_text+has_ans.answer_text_in_prediction # logical OR operator\n",
    "has_ans= has_ans.drop(has_ans[['answer_text_in_prediction','answer_prediction_in_answer_text']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_ans ['question']= has_ans ['question'].str.replace('[^\\w\\s]','')\n",
    "has_ans ['question']= has_ans ['question'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_ans = has_ans[has_ans['TF'].notna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_ans['question_type']=''\n",
    "keywords = [\"why\", \"when\", \"how\", \"what\", \"who\", \"where\",'which']\n",
    "for  (i, answer) in enumerate(has_ans['question']):\n",
    "    words = answer.split()\n",
    "    for key in keywords:\n",
    "        if key in  words:\n",
    "            has_ans['question_type'][i]=key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_ans['question_type']=has_ans['question_type'].replace('', 'other')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_ans.question_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_ans['question_type']=has_ans['question_type'].replace('', 'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_ans.question_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_ans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "  <li>context - provided context</li>\n",
    "  <li>id - ID of question</li>\n",
    "  <li>TF - whether the prediction was correct or not (True/False)</li>\n",
    "  <li>pred_answer - what was the predicted answer</li>\n",
    "  <li>possible_answers - acceptable answers</li>\n",
    "  <li>question - question ansked</li>\n",
    "  <li>TF_F1 - approx. correct answers according to F1 score</li>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corresponds with the scores: \n",
    "<ul>\n",
    "    <li> \"exact\": 53.56174990321332,</li>\n",
    "    <li>\"f1\": 69.15141871385678, </li>\n",
    "    <li>\"total\": 5166 , </li>\n",
    "    <li>\"HasAns_exact\": 53.56174990321332, </li>\n",
    "    <li>\"HasAns_f1\": 69.15141871385678, </li>\n",
    "    <li>\"HasAns_total\": 5166,</li>\n",
    "    <li>\"best_exact\": 53.56174990321332,</li>\n",
    "    <li>\"best_exact_thresh\": -11.604235649108887, </li>\n",
    "    <li>\"best_f1\":  69.15141871385688, </li>\n",
    "    <li>\"best_f1_thresh\": -8.893298149108887</li>\n",
    " </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_ans.TF_F1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_ans.TF.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def plot_histogram(data, label='',datalabel=''):\n",
    "    plt.hist(data, bins=100, normed=1, alpha=1.0)\n",
    "    plt.xlabel('Lengths')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.grid(True)\n",
    "    plt.title((datalabel+' '+label+' length').capitalize())\n",
    "    plt.savefig('graphs/'+datalabel+'_length_'+label+'.png')\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram([(len(str(a))) for a in has_ans['pred_answer']], 'answer','has_ans')\n",
    "plot_histogram([q for q in has_ans['question'].str.len()], 'question','has_ans')\n",
    "plot_histogram([c for c in has_ans['context'].str.len()], 'context','has_ans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_ans['ans_length']  = has_ans['pred_answer'].str.len()\n",
    "has_ans['con_length']  = has_ans['context'].str.len()\n",
    "has_ans['que_length']  = has_ans['question'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_questiontype(dataset,label,x,datalabel):\n",
    "    answers= []\n",
    "    keywords = [\"why\", \"when\", \"how\", \"what\", \"who\", \"where\",'which','other']\n",
    "    for key in keywords:\n",
    "        answers.append([len(answer) for (i, answer) in enumerate(dataset) if key in has_ans.iloc[i]['question_type'].lower()])\n",
    "    bins = np.linspace(0, x, 15)\n",
    "    plt.hist(answers, bins, normed=True, label=keywords)\n",
    "    plt.xlabel('Lengths')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.title((datalabel+' '+label+' questiontype').capitalize())\n",
    "    plt.savefig('graphs/'+datalabel+'_questiontype_'+label+'.png')\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_questiontype(has_ans['pred_answer'],'answer',100,'has_ans' )\n",
    "question_questiontype(has_ans['context'],'context',3000,'has_ans')\n",
    "question_questiontype(has_ans['question'],'question',150,'has_ans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect=has_ans[has_ans['TF'] == False]\n",
    "correct=has_ans[has_ans['TF']==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only incorrect answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram([a for a in incorrect['pred_answer'].str.len()], 'answer','has_ans_incorrect')\n",
    "plot_histogram([q for q in incorrect['question'].str.len()], 'question','has_ans_incorrect')\n",
    "plot_histogram([c for c in incorrect['context'].str.len()], 'context','has_ans_incorrect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_questiontype(incorrect['pred_answer'],'answer',100,'has_ans_incorrect' )\n",
    "question_questiontype(incorrect['context'],'context',3000,'has_ans_incorrect')\n",
    "question_questiontype(incorrect['question'],'question',150,'has_ans_incorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only correct answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram([a for a in correct['pred_answer'].str.len()], 'answer','has_ans_correct')\n",
    "plot_histogram([q for q in correct['question'].str.len()], 'question','has_ans_correct')\n",
    "plot_histogram([c for c in correct['context'].str.len()], 'context','has_ans_correct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_questiontype(correct['pred_answer'],'answer',100,'has_ans_correct' )\n",
    "question_questiontype(correct['context'],'context',3000,'has_ans_correct')\n",
    "question_questiontype(correct['question'],'question',150,'has_ans_correct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=has_ans, x=\"ans_length\",bins=100, hue='TF',palette=[\"C0\", \"C1\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pie charts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def piechart (dataset,labels,name):\n",
    "    values=dataset.value_counts()\n",
    "    labels=dataset.value_counts().index.values\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.pie(values,labels=labels,autopct='%1.1f%%',shadow=True)\n",
    "    plt.legend(labels)\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.title((name+' question type distribution').capitalize())\n",
    "    plt.savefig('graphs/'+name+'_questiontype_pie.png')\n",
    "    plt.show()\n",
    "    print(dataset.value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=''\n",
    "piechart(has_ans['question_type'],labels,'has_ans')\n",
    "piechart(incorrect['question_type'],labels,'has_ans_incorrect')\n",
    "piechart(correct['question_type'],labels,'has_ans_correct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With the F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect=has_ans[has_ans['TF_F1'] == False]\n",
    "correct=has_ans[has_ans['TF_F1']==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only incorrect answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram([a for a in incorrect['pred_answer'].str.len()], 'answer','has_ans_incorrect')\n",
    "plot_histogram([q for q in incorrect['question'].str.len()], 'question','has_ans_incorrect')\n",
    "plot_histogram([c for c in incorrect['context'].str.len()], 'context','has_ans_incorrect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_questiontype(incorrect['pred_answer'],'answer',100,'has_ans_incorrect' )\n",
    "question_questiontype(incorrect['context'],'context',3000,'has_ans_incorrect')\n",
    "question_questiontype(incorrect['question'],'question',150,'has_ans_incorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only correct answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram([a for a in correct['pred_answer'].str.len()], 'answer','has_ans_correct')\n",
    "plot_histogram([q for q in correct['question'].str.len()], 'question','has_ans_correct')\n",
    "plot_histogram([c for c in correct['context'].str.len()], 'context','has_ans_correct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_questiontype(correct['pred_answer'],'answer',100,'has_ans_correct' )\n",
    "question_questiontype(correct['context'],'context',3000,'has_ans_correct')\n",
    "question_questiontype(correct['question'],'question',150,'has_ans_correct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=has_ans, x=\"ans_length\",bins=100, hue='TF',palette=[\"C0\", \"C1\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pie charts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=''\n",
    "piechart(has_ans['question_type'],labels,'has_ans')\n",
    "piechart(incorrect['question_type'],labels,'has_ans_incorrect')\n",
    "piechart(correct['question_type'],labels,'has_ans_correct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crosstab\n",
    "df=has_ans\n",
    "pal = [\"red\",\"royalblue\" ]\n",
    "ax= pd.crosstab(df['question_type'],df['TF'] ).apply(lambda r: r/r.sum()*100, axis=1)\n",
    "ax=ax.sort_values([1], ascending=True)\n",
    "ax_1 = ax.plot.bar(figsize=(10,6),stacked=True, rot=0,color=pal, sort_columns=True)\n",
    "display(ax)\n",
    "\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(1.0,1.0), title=\"Correct Prediction\")\n",
    "\n",
    "plt.xlabel('Question type')\n",
    "plt.ylabel('Percent Distribution')\n",
    "\n",
    "for rec in ax_1.patches:\n",
    "    height = rec.get_height()\n",
    "    ax_1.text(rec.get_x() + rec.get_width() / 2, \n",
    "              rec.get_y() + height / 2,\n",
    "              \"{:.0f}%\".format(height),\n",
    "              ha='center', \n",
    "              va='bottom')\n",
    "plt.savefig('graphs/NewsQA_questiontype_distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "how_analysis=has_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_analysis=how_analysis.reset_index(drop=True)\n",
    "how_analysis['question']=how_analysis['question'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_analysis.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_analysis['question_type']=''\n",
    "qty=[]\n",
    "keywords = ['how','far','long','many', 'much','old', 'come']\n",
    "for index, row in how_analysis.iterrows():\n",
    "\n",
    "    answer=row['question']\n",
    "    words = answer.split()\n",
    "    for key in keywords:\n",
    "        if key in  words:\n",
    "            qty.append(key)\n",
    "    how_analysis['question_type'][index]=qty\n",
    "    qty=[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_analysis=how_analysis[how_analysis.question_type.apply(lambda x: len(x) > 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_analysis.question_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_analysis=how_analysis[how_analysis['question_type']!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_analysis['question_type'] = how_analysis['question_type'].str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh=how_analysis[how_analysis['question_type']=='many']\n",
    "gh.sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crosstab\n",
    "df=how_analysis\n",
    "pal = [\"red\",\"royalblue\" ]\n",
    "ax= pd.crosstab(df['question_type'],df['TF'] ).apply(lambda r: r/r.sum()*100, axis=1)\n",
    "ax=ax.sort_values([1], ascending=True)\n",
    "ax_1 = ax.plot.bar(figsize=(10,6),stacked=True, rot=0,color=pal, sort_columns=True)\n",
    "display(ax)\n",
    "\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(1.0, 1.0), title=\"Correct Prediction\")\n",
    "\n",
    "plt.xlabel('Question type')\n",
    "plt.ylabel('Percent Distribution')\n",
    "\n",
    "for rec in ax_1.patches:\n",
    "    height = rec.get_height()\n",
    "    ax_1.text(rec.get_x() + rec.get_width() / 2, \n",
    "              rec.get_y() + height / 2,\n",
    "              \"{:.0f}%\".format(height),\n",
    "              ha='center', \n",
    "              va='bottom')\n",
    "plt.savefig('graphs/how_questiontype_distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_ans[has_ans['question_type']=='other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERROR ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predicted answer is longer than the original but still conveys correct information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc=has_ans[has_ans['TF'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc['answer_text_in_prediction'] = inc.apply(lambda x: x.possible_answer in x.pred_answer, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc['possible_answer_length']  = inc['possible_answer'].str.len()\n",
    "inc['difference'] = inc.apply(lambda x: x.possible_answer_length < x.ans_length, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc['kkt'] = inc.apply(lambda x: x.answer_text_in_prediction &  x.difference, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc.kkt.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking whether the predicted answer is shorter than the original answer but still conveys same information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc['answer_prediction_in_answer_text'] = inc.apply(lambda x: x.pred_answer in x.possible_answer, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc['difference2'] = inc.apply(lambda x: x.possible_answer_length > x.ans_length, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc['kkt2'] = inc.apply(lambda x: x.answer_prediction_in_answer_text &  x.difference2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc.kkt2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# getting examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#will result in error since tehre are none longer and shorter at the same time\n",
    "\n",
    "#example=inc[(inc['kkt'] == True)& (inc['kkt2'] == True)]  \n",
    "#example.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example.kkt.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc=inc[((inc['kkt'] & inc['kkt2']) == False)]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#longer\n",
    "longer=inc[~(inc['kkt'] == False)]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longer.kkt.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longer.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piechart(longer['question_type'],labels,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shorter\n",
    "shorter=inc[~(inc['kkt2'] == False)]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorter.kkt2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorter.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piechart(shorter['question_type'],labels,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wrong predictions\n",
    "wrong=inc[~(inc['kkt2'] == True )& ~(inc['kkt'] == True )]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wrong.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piechart(wrong['question_type'],labels,'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis of char-count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answ=has_ans\n",
    "def analyse_lowest(part, label):\n",
    "    se=100000\n",
    "    for q in part.str.len():\n",
    "        if se>q:\n",
    "            se=q\n",
    "            \n",
    "    print('The length of ',label,'starts at ',se)\n",
    "def analyse_highest(part, label):\n",
    "    se=0\n",
    "    for q in part.str.len():\n",
    "        if se<q:\n",
    "            se=q\n",
    "            \n",
    "    print('The length of ',label,'ends at ',se)\n",
    "answ = answ[answ['possible_answer'].notna()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_lowest(answ['possible_answer'],'original answer')\n",
    "analyse_lowest(answ['context'],'context')\n",
    "analyse_lowest(answ['question'],'question')\n",
    "analyse_lowest(answ['pred_answer'],'pred_answer')\n",
    "analyse_highest(answ['possible_answer'],'original answer')\n",
    "analyse_highest(answ['context'],'context')\n",
    "analyse_highest(answ['question'],'question')\n",
    "analyse_highest(answ['pred_answer'],'pred_answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allquestiontype pairs \n",
    "has_ans['question_type']=''\n",
    "qty=[]\n",
    "keywords = [\"why\", \"when\", \"how\", \"what\", \"who\", \"where\",'which', 'whom','whose','does']\n",
    "for index, row in has_ans.iterrows():\n",
    "\n",
    "    answer=row['question']\n",
    "    words = answer.split()\n",
    "    for key in keywords:\n",
    "        if key in  words:\n",
    "            qty.append(key)\n",
    "    has_ans['question_type'][index]=qty\n",
    "    qty=[]\n",
    "has_ans.question_type.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exports for manual analysis of the wrong answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export=wrong[['pred_answer','possible_answer','question','id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export.to_json('kek.json',orient=\"records\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
