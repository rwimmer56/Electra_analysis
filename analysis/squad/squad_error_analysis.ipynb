{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_ans= pd.read_csv('has_answer_squad.csv')\n",
    "no_ans= pd.read_csv('no_answer_squad.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_ans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_ans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_ans= has_ans.drop(has_ans.columns[0], axis=1)\n",
    "has_ans= has_ans.drop(has_ans.columns[6], axis=1)\n",
    "no_ans= no_ans.drop(no_ans.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_ans ['question']= has_ans ['question'].str.replace('[^\\w\\s]','')\n",
    "no_ans ['question']= no_ans ['question'].str.replace('[^\\w\\s]','')\n",
    "has_ans ['question']= has_ans ['question'].str.lower()\n",
    "no_ans ['question']= no_ans ['question'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_ans = has_ans[has_ans['TF'].notna()]\n",
    "no_ans = no_ans[no_ans['TF'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_ans['question_type']=''\n",
    "qty=[]\n",
    "keywords = [\"why\", \"when\", \"how\", \"what\", \"who\", \"where\",'which']\n",
    "for index, row in has_ans.iterrows():\n",
    "    answer=row['question']\n",
    "    words = answer.split()\n",
    "    for key in keywords:\n",
    "        if key in  words:\n",
    "            qty.append(key)\n",
    "    row['question_type']=qty\n",
    "    qty=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_ans.head(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_ans.question_type = has_ans.question_type.apply(lambda y: '' if len(y)==0 else y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_ans['question_type']=has_ans['question_type'].replace('', 'other')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_ans[\"question_type\"] = has_ans[\"question_type\"].str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_ans['question_type']=has_ans['question_type'].replace('o', 'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_ans.question_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_ans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "  <li>context - provided context</li>\n",
    "  <li>id - ID of question</li>\n",
    "  <li>TF - whether the prediction was correct or not (True/False)</li>\n",
    "  <li>pred_answer - what was the predicted answer</li>\n",
    "  <li>possible_answers - acceptable answers</li>\n",
    "  <li>question - question ansked</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_ans['question_type']=''\n",
    "keywords = [\"why\", \"when\", \"how\", \"what\", \"who\", \"where\",'which', 'whom','whose','does']\n",
    "for  (i, answer) in enumerate(no_ans['question']):\n",
    "    words = answer.split()\n",
    "    for key in keywords:\n",
    "        if key in words:\n",
    "            no_ans['question_type'][i]=key\n",
    "            \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_ans['question_type']=no_ans['question_type'].replace('', 'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_ans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "  <li>id - ID of question</li>\n",
    "  <li>TF - whether the prediction was correct or not (True/False)</li>\n",
    "  <li>answer - what was the predicted answer</li>\n",
    "  <li>question - question ansked</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_ans.TF.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_ans.TF.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corresponds with the scores: \n",
    "<ul>\n",
    "    <li> \"exact\": 86.8441000589573,</li>\n",
    "    <li>\"f1\": 89.48714305212785, </li>\n",
    "    <li>\"total\": 11873, </li>\n",
    "    <li>\"HasAns_exact\": 82.08502024291498, </li>\n",
    "    <li>\"HasAns_f1\": 87.37868580599083, </li>\n",
    "    <li>\"HasAns_total\": 5928,</li>\n",
    "    <li>\"NoAns_exact\": 91.58957106812447, </li>\n",
    "    <li>\"NoAns_f1\": 91.58957106812447, </li>\n",
    "    <li>\"NoAns_total\": 5945, </li>\n",
    "    <li>\"best_exact\": 86.86936747241641,</li>\n",
    "    <li>\"best_exact_thresh\": -2.737598419189453, </li>\n",
    "    <li>\"best_f1\": 89.51241046558647, </li>\n",
    "    <li>\"best_f1_thresh\": -2.737598419189453</li>\n",
    " </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_ans = no_ans.rename({'answer': 'pred_answer'}, axis='columns')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def plot_histogram(data, label='',datalabel=''):\n",
    "    plt.hist(data, bins=100, normed=1, alpha=1.0)\n",
    "    plt.xlabel('Lengths')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.grid(True)\n",
    "    plt.title((datalabel+' '+label+' length').capitalize())\n",
    "    plt.savefig('graphs/'+datalabel+'_length_'+label+'.png')\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram([(len(str(a))) for a in has_ans['pred_answer']], 'answer','has_ans')\n",
    "plot_histogram([q for q in has_ans['question'].str.len()], 'question','has_ans')\n",
    "plot_histogram([c for c in has_ans['context'].str.len()], 'context','has_ans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_ans['ans_length']  = has_ans['pred_answer'].str.len()\n",
    "has_ans['con_length']  = has_ans['context'].str.len()\n",
    "has_ans['que_length']  = has_ans['question'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_questiontype(dataset,label,x,datalabel):\n",
    "    answers= []\n",
    "    keywords = [\"why\", \"when\", \"how\", \"what\", \"who\", \"where\",'which','other']\n",
    "    for key in keywords:\n",
    "        answers.append([len(answer) for (i, answer) in enumerate(dataset) if key in has_ans.iloc[i]['question_type'].lower()])\n",
    "    bins = np.linspace(0, x, 15)\n",
    "    plt.hist(answers, bins, normed=True, label=keywords)\n",
    "    plt.xlabel('Lengths')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.title((datalabel+' '+label+' questiontype').capitalize())\n",
    "    plt.savefig('graphs/'+datalabel+'_questiontype_'+label+'.png')\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_questiontype(has_ans['pred_answer'],'answer',100,'has_ans' )\n",
    "question_questiontype(has_ans['context'],'context',3000,'has_ans')\n",
    "question_questiontype(has_ans['question'],'question',150,'has_ans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_ans.question_type.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect=has_ans[has_ans['TF'] == False]\n",
    "correct=has_ans[has_ans['TF']==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only incorrect answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram([a for a in incorrect['pred_answer'].str.len()], 'answer','has_ans_incorrect')\n",
    "plot_histogram([q for q in incorrect['question'].str.len()], 'question','has_ans_incorrect')\n",
    "plot_histogram([c for c in incorrect['context'].str.len()], 'context','has_ans_incorrect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_questiontype(incorrect['pred_answer'],'answer',100,'has_ans_incorrect' )\n",
    "question_questiontype(incorrect['context'],'context',3000,'has_ans_incorrect')\n",
    "question_questiontype(incorrect['question'],'question',150,'has_ans_incorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only correct answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram([a for a in correct['pred_answer'].str.len()], 'answer','has_ans_correct')\n",
    "plot_histogram([q for q in correct['question'].str.len()], 'question','has_ans_correct')\n",
    "plot_histogram([c for c in correct['context'].str.len()], 'context','has_ans_correct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_questiontype(correct['pred_answer'],'answer',100,'has_ans_correct' )\n",
    "question_questiontype(correct['context'],'context',3000,'has_ans_correct')\n",
    "question_questiontype(correct['question'],'question',150,'has_ans_correct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=has_ans, x=\"ans_length\",bins=100, hue='TF',palette=[\"C0\", \"C1\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct=has_ans[has_ans['TF']==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pie charts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def piechart (dataset,labels,name):\n",
    "    values=dataset.value_counts()\n",
    "    labels=dataset.value_counts().index.values\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.pie(values,labels=labels,autopct='%1.1f%%',shadow=True)\n",
    "    plt.legend(labels)\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.title((name+' question type distribution').capitalize())\n",
    "    plt.savefig('graphs/'+name+'_questiontype_pie.png')\n",
    "    plt.show()\n",
    "    print(dataset.value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=''\n",
    "piechart(has_ans['question_type'],labels,'has_ans')\n",
    "piechart(incorrect['question_type'],labels,'has_ans_incorrect')\n",
    "piechart(correct['question_type'],labels,'has_ans_correct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pie_in(labels,dataset,name,feature):\n",
    "    plt.figure(figsize=(4,4))\n",
    "    values=dataset.value_counts().values\n",
    "    labels=dataset.value_counts().index.values\n",
    "    plt.pie(values,labels=labels,autopct='%1.1f%%',shadow=True)\n",
    "    plt.legend(labels)\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.title((name+' '+feature+' attribute distribution').capitalize())\n",
    "    plt.savefig('graphs/'+name+'_'+feature+'_pie.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unanswerable part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram([(len(str(a))) for a in no_ans['pred_answer']], 'answer','no_ans')\n",
    "plot_histogram([q for q in no_ans['question'].str.len()], 'question','no_ans')\n",
    "plot_histogram([c for c in no_ans['context'].str.len()], 'context','no_ans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_ans['ans_length']  = no_ans['pred_answer'].str.len()\n",
    "no_ans['con_length']  = no_ans['context'].str.len()\n",
    "no_ans['que_length']  = no_ans['question'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_questiontype(dataset,label,x,datalabel):\n",
    "    answers= []\n",
    "    keywords = [\"why\", \"when\", \"how\", \"what\", \"who\", \"where\",'which','other']\n",
    "    for key in keywords:\n",
    "        answers.append([len(answer) for (i, answer) in enumerate(dataset) if key in no_ans.iloc[i]['question_type'].lower()])\n",
    "    bins = np.linspace(0, x, 15)\n",
    "    plt.hist(answers, bins, normed=True, label=keywords)\n",
    "    plt.xlabel('Lengths')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.title((label+' questiontype').capitalize())\n",
    "    plt.savefig('graphs/'+datalabel+'_questiontype_'+label+'.png')\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_questiontype(no_ans['pred_answer'],'answer',100,'no_ans' )\n",
    "question_questiontype(no_ans['context'],'context',3000,'no_ans')\n",
    "question_questiontype(no_ans['question'],'question',150,'no_ans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect=no_ans[no_ans['TF'] == False]\n",
    "correct=no_ans[no_ans['TF']==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only incorrect answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram([a for a in incorrect['pred_answer'].str.len()], 'answer','no_ans_incorrect')\n",
    "plot_histogram([q for q in incorrect['question'].str.len()], 'question','no_ans_incorrect')\n",
    "plot_histogram([c for c in incorrect['context'].str.len()], 'context','no_ans_incorrect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_questiontype(incorrect['pred_answer'],'answer',100,'no_ans_incorrect' )\n",
    "question_questiontype(incorrect['context'],'context',3000,'no_ans_incorrect')\n",
    "question_questiontype(incorrect['question'],'question',150,'no_ans_incorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only correct answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram([a for a in correct['pred_answer'].str.len()], 'answer','no_ans_correct')\n",
    "plot_histogram([q for q in correct['question'].str.len()], 'question','no_ans_correct')\n",
    "plot_histogram([c for c in correct['context'].str.len()], 'context','no_ans_correct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_questiontype(correct['pred_answer'],'answer',100,'no_ans_correct' )\n",
    "question_questiontype(correct['context'],'context',3000,'no_ans_correct')\n",
    "question_questiontype(correct['question'],'question',150,'no_ans_correct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "piechart(no_ans['question_type'],labels,'no_ans')\n",
    "piechart(incorrect['question_type'],labels,'no_ans_incorrect')\n",
    "piechart(correct['question_type'],labels,'no_ans_correct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans=pd.merge(has_ans,no_ans,how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram([(len(str(a))) for a in ans['pred_answer']], 'answer','ans')\n",
    "plot_histogram([q for q in ans['question'].str.len()], 'question','ans')\n",
    "plot_histogram([c for c in ans['context'].str.len()], 'context','ans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['ans_length']  = ans['pred_answer'].str.len()\n",
    "ans['con_length']  = ans['context'].str.len()\n",
    "ans['que_length']  = ans['question'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_questiontype(dataset,label,x,datalabel):\n",
    "    answers= []\n",
    "    keywords = [\"why\", \"when\", \"how\", \"what\", \"who\", \"where\",'which','other']\n",
    "    for key in keywords:\n",
    "        answers.append([len(answer) for (i, answer) in enumerate(dataset) if key in ans.iloc[i]['question_type'].lower()])\n",
    "    bins = np.linspace(0, x, 15)\n",
    "    plt.hist(answers, bins, normed=True, label=keywords)\n",
    "    plt.xlabel('Lengths')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.title((label+' questiontype').capitalize())\n",
    "    plt.savefig('graphs/'+datalabel+'_questiontype_'+label+'.png')\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_questiontype(ans['pred_answer'],'answer',100,'ans' )\n",
    "question_questiontype(ans['context'],'context',3000,'ans')\n",
    "question_questiontype(ans['question'],'question',150,'ans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect=ans[ans['TF'] == False]\n",
    "correct=ans[ans['TF']==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only incorrect answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram([a for a in incorrect['pred_answer'].str.len()], 'answer','ans_incorrect')\n",
    "plot_histogram([q for q in incorrect['question'].str.len()], 'question','ans_incorrect')\n",
    "plot_histogram([c for c in incorrect['context'].str.len()], 'context','ans_incorrect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_questiontype(incorrect['pred_answer'],'answer',100,'ans_incorrect' )\n",
    "question_questiontype(incorrect['context'],'context',3000,'ans_incorrect')\n",
    "question_questiontype(incorrect['question'],'question',150,'ans_incorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only correct answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram([a for a in correct['pred_answer'].str.len()], 'answer','ans_correct')\n",
    "plot_histogram([q for q in correct['question'].str.len()], 'question','ans_correct')\n",
    "plot_histogram([c for c in correct['context'].str.len()], 'context','ans_correct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_questiontype(correct['pred_answer'],'answer',100,'ans_correct' )\n",
    "question_questiontype(correct['context'],'context',3000,'ans_correct')\n",
    "question_questiontype(correct['question'],'question',150,'ans_correct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piechart(ans['question_type'],labels,'SQUAD Dataset')\n",
    "piechart(incorrect['question_type'],labels,'SQUAD - Incorrect predictions')\n",
    "piechart(correct['question_type'],labels,'SQUAD - Correct predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# question type analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crosstab\n",
    "df=ans.sort_values('TF')\n",
    "pal = [\"red\",\"royalblue\" ]\n",
    "ax= pd.crosstab(df['question_type'],df['TF'] ).apply(lambda r: r/r.sum()*100, axis=1)\n",
    "ax=ax.sort_values([1], ascending=True)\n",
    "ax_1 = ax.plot.bar(figsize=(10,6),stacked=True, rot=0,color=pal, sort_columns=True)\n",
    "display(ax)\n",
    "\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(1.0, 1.0), title=\"Correct Prediction\")\n",
    "\n",
    "plt.xlabel('Question type')\n",
    "plt.ylabel('Percent Distribution')\n",
    "\n",
    "for rec in ax_1.patches:\n",
    "    height = rec.get_height()\n",
    "    ax_1.text(rec.get_x() + rec.get_width() / 2, \n",
    "              rec.get_y() + height / 2,\n",
    "              \"{:.0f}%\".format(height),\n",
    "              ha='center', \n",
    "              va='bottom')\n",
    "plt.savefig('graphs/SQUAD_questiontype_distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.sort_values([1], ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "how_analysis=pd.merge(has_ans,no_ans,how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_analysis=how_analysis.reset_index(drop=True)\n",
    "how_analysis['question']=how_analysis['question'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_analysis.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_analysis['question_type']=''\n",
    "qty=[]\n",
    "keywords = ['how','far','long','many', 'much','old', 'come']\n",
    "for index, row in how_analysis.iterrows():\n",
    "\n",
    "    answer=row['question']\n",
    "    words = answer.split()\n",
    "    for key in keywords:\n",
    "        if key in  words:\n",
    "            qty.append(key)\n",
    "    how_analysis['question_type'][index]=qty\n",
    "    qty=[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_analysis=how_analysis[how_analysis.question_type.apply(lambda x: len(x) > 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_analysis.question_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_analysis=how_analysis[how_analysis['question_type']!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_analysis['question_type'] = how_analysis['question_type'].str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh=how_analysis[how_analysis['question_type']=='many']\n",
    "gh.sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crosstab\n",
    "df=how_analysis\n",
    "pal = [\"red\",\"royalblue\" ]\n",
    "ax= pd.crosstab(df['question_type'],df['TF'] ).apply(lambda r: r/r.sum()*100, axis=1)\n",
    "ax=ax.sort_values([1], ascending=True)\n",
    "ax_1 = ax.plot.bar(figsize=(10,6),stacked=True, rot=0,color=pal, sort_columns=True)\n",
    "display(ax)\n",
    "\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(1.0, 1.0), title=\"Correct Prediction\")\n",
    "\n",
    "plt.xlabel('Question type')\n",
    "plt.ylabel('Percent Distribution')\n",
    "\n",
    "for rec in ax_1.patches:\n",
    "    height = rec.get_height()\n",
    "    ax_1.text(rec.get_x() + rec.get_width() / 2, \n",
    "              rec.get_y() + height / 2,\n",
    "              \"{:.0f}%\".format(height),\n",
    "              ha='center', \n",
    "              va='bottom')\n",
    "plt.savefig('graphs/how_questiontype_distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERROR ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# incorrectly preddicted NoAns where answer existed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect.loc[~incorrect.pred_answer.isin(['-']), 'pred_answer'] = 'Other'\n",
    "incorrect.pred_answer.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noanswrg=incorrect[(incorrect['pred_answer'] == '-')]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "piechart(noanswrg['question_type'],labels,'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# question is unasnwerable but the answer was predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect.possible_answers.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unans=incorrect[incorrect.possible_answers.isnull()==True]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piechart(unans['question_type'],labels,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#restoring to original\n",
    "incorrect=ans[ans['TF'] == False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for copying examples\n",
    "incorrect[~(incorrect['possible_answers'] == 'NaN')]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc=ans[ans['TF'] == False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the previous two groups\n",
    "inc=inc[~(inc['pred_answer'] == '-')]  \n",
    "inc = inc[inc['possible_answers'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing if everything is gone\n",
    "inc.possible_answers.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc[(inc['pred_answer'] == '-')]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preparation for the upcoming checks whether the predicted answer contains the original answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exploding possible answers since there may be more than one\n",
    "inc['possible_answers'] = inc['possible_answers'].str.replace('[', '').astype(str)\n",
    "inc['possible_answers'] = inc['possible_answers'].str.replace(']', '').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc['possible']=inc['possible_answers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc= inc.assign(possible=inc['possible'].str.split('|')).explode('possible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if the original answer is present in the predicted anser\n",
    "#(that means the model picked longer sequence of words than the original answer.\n",
    "#the information conveyed is still correct.)\n",
    "inc['answer_text_in_prediction'] = inc.apply(lambda x: x.possible in x.pred_answer, axis=1)\n",
    "inc['answer_prediction_in_answer_text'] = inc.apply(lambda x: x.pred_answer in x.possible_answers, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#caring only about the ones that are longer\n",
    "inc['poss_length']  = inc['possible'].str.len()\n",
    "inc['difference'] = inc.apply(lambda x: x.poss_length < x.ans_length, axis=1)\n",
    "inc['difference2'] = inc.apply(lambda x: x.poss_length > x.ans_length, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predicted answer is longer than the original but still conveys correct information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouping back after the necessary checks\n",
    "inc=inc.groupby('id').agg(lambda x: x.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating attribute to check whether if there are such instances \n",
    "inc['kkt'] = inc.apply(lambda x: any(x.answer_text_in_prediction) &  any(x.difference), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc.kkt.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking whether the predicted answer is shorter than the original answer but still conveys same information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc['kkt2'] = inc.apply(lambda x: any(x.answer_prediction_in_answer_text) &  any(x.difference2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inc.kkt2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# since there are several possible answers for one question, its important to drop instances where some plausible answer can fulfil the condition of being longer than predicted answer and the other plausible answer for the same question fulfilling condition of beingh shorter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example=inc[(inc['kkt'] == True)& (inc['kkt2'] == True)]  \n",
    "example.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example.kkt.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc=inc[((inc['kkt'] & inc['kkt2']) == False)]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge=pd.merge(incorrect,inc,how='right', on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longer=merge[(merge['kkt'] == True)]\n",
    "longer.kkt.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorter=merge[(merge['kkt2'] == True)]  \n",
    "shorter.kkt2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piechart(shorter['question_type_x'],labels,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piechart(longer['question_type_x'],labels,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longer.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorter.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# just wrong answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong=merge[(merge['kkt2'] == False)&(merge['kkt'] == False)]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piechart(wrong['question_type_x'],labels,'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis of char-count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answ=ans\n",
    "def analyse_lowest(part, label):\n",
    "    se=100000\n",
    "    for q in part.str.len():\n",
    "        if se>q:\n",
    "            se=q\n",
    "            \n",
    "    print('The length of ',label,'starts at ',se)\n",
    "def analyse_highest(part, label):\n",
    "    se=0\n",
    "    for q in part.str.len():\n",
    "        if se<q:\n",
    "            se=q\n",
    "            \n",
    "    print('The length of ',label,'ends at ',se)\n",
    "answ = answ[answ['possible_answers'].notna()]\n",
    "#exploding possible answers\n",
    "answ['possible_answers'] = answ['possible_answers'].str.replace('[', '').astype(str)\n",
    "answ['possible_answers'] = answ['possible_answers'].str.replace(']', '').astype(str)\n",
    "answ['possible']=answ['possible_answers']    \n",
    "answ= answ.assign(possible=answ['possible'].str.split('|')).explode('possible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_lowest(answ['possible'],'original answer')\n",
    "analyse_lowest(answ['context'],'context')\n",
    "analyse_lowest(answ['question'],'question')\n",
    "analyse_lowest(answ['pred_answer'],'pred_answer')\n",
    "analyse_highest(answ['possible'],'original answer')\n",
    "analyse_highest(answ['context'],'context')\n",
    "analyse_highest(answ['question'],'question')\n",
    "analyse_highest(answ['pred_answer'],'pred_answer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# questiontype samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_ans[has_ans['question_type']=='other']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wh=no_ans[no_ans['question_type']=='other']\n",
    "wh.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# distribution of all questiontype pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ans['question_type']=''\n",
    "qty=[]\n",
    "keywords = [\"why\", \"when\", \"how\", \"what\", \"who\", \"where\",'which', 'whom','whose','does']\n",
    "for index, row in ans.iterrows():\n",
    "\n",
    "    answer=row['question']\n",
    "    words = answer.split()\n",
    "    for key in keywords:\n",
    "        if key in  words:\n",
    "            qty.append(key)\n",
    "    ans['question_type'][index]=qty\n",
    "    qty=[]\n",
    "ans.question_type.value_counts()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
